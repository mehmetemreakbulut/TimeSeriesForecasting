{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1816ee51-aa1d-4cf2-9dc4-a71160cef600",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5da24ba9-a7cf-4f65-b7f8-136c08822259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1MAXmmYzW1WUUDyNZBCiZNdP2njZFHUL0\n",
      "From (redirected): https://drive.google.com/uc?id=1MAXmmYzW1WUUDyNZBCiZNdP2njZFHUL0&confirm=t&uuid=e2509643-4440-40f6-b0ce-dee3b238f200\n",
      "To: /home/ubuntu/hive_retrieval_engine/dataset.zip\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 56.4M/56.4M [00:00<00:00, 65.8MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./dataset.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  inflating: categories.npy          \n",
      "  inflating: training_data.npy       \n",
      "  inflating: valid_periods.npy       \n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "file_id = '1MAXmmYzW1WUUDyNZBCiZNdP2njZFHUL0'\n",
    "file_path = './dataset.zip'\n",
    "\n",
    "gdown.download(f'https://drive.google.com/uc?id={file_id}', file_path, quiet=False)\n",
    "!unzip ./dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "447550f8-8d90-4979-b3fa-5f2823e3b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "categories = np.load('./categories.npy')\n",
    "valid_periods = np.load('./valid_periods.npy')\n",
    "training_data = np.load('./training_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9081c0b5-8b47-4261-ba76-d28dc6d651c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = categories == 'B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4bb997e-c990-4de9-ac60-4752930e6d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_periods = valid_periods[mask]\n",
    "training_data = training_data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e46006e9-0a44-4d1d-b3bf-ff28928649ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10987 entries, 0 to 10986\n",
      "Columns: 2776 entries, 0 to 2775\n",
      "dtypes: float64(2776)\n",
      "memory usage: 232.7 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "training_data_df = pd.DataFrame(training_data)\n",
    "training_data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "241b6c88-d520-4cea-a108-319cf667be2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8789, 2776), (8789, 2), (2198, 2776), (2198, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the indices of the time series into training and validation sets\n",
    "train_indices, val_indices = train_test_split(np.arange(len(training_data_df)), test_size=0.2, random_state=42)\n",
    "\n",
    "# Use these indices to create training and validation sets\n",
    "train_set = training_data_df.iloc[train_indices]\n",
    "valid_periods_train = valid_periods[train_indices]\n",
    "\n",
    "val_set = training_data_df.iloc[val_indices]\n",
    "valid_periods_val = valid_periods[val_indices]\n",
    "\n",
    "train_set.shape, valid_periods_train.shape, val_set.shape, valid_periods_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdda2085-e770-433b-b102-968a0622b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_padding_length(sq, w, t, s):\n",
    "    cycle = w + t\n",
    "    if sq <= cycle: return cycle - sq\n",
    "    else:\n",
    "        m = (sq - cycle) % s\n",
    "        if m == 0: return m\n",
    "        else: return s - m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37585e31-4d6c-4301-93b0-7853edecebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, valid_periods, window, telescope, stride=1):\n",
    "\n",
    "    input_sequences = []\n",
    "    input_categories = []\n",
    "    output_sequences = []\n",
    "\n",
    "\n",
    "    for i in range(len(valid_periods)):\n",
    "        start, end = valid_periods[i]\n",
    "\n",
    "        sequence = df.iloc[i, start:end].values\n",
    "\n",
    "        padding_length = calculate_padding_length(len(sequence), window, telescope, stride)\n",
    "        sequence = np.pad(sequence, (padding_length, 0), mode='constant', constant_values=0)\n",
    "\n",
    "        for j in range(0, len(sequence) - window - telescope + 1, stride):\n",
    "            input_seq = sequence[j:(j + window)]\n",
    "            output_seq = sequence[(j + window):(j + window + telescope)]\n",
    "\n",
    "            input_sequences.append(input_seq)\n",
    "            output_sequences.append(output_seq)\n",
    "\n",
    "    return np.array(input_sequences), np.array(output_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7624c92-a086-4445-b957-f8b268bc1089",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 200\n",
    "telescope = 9\n",
    "stride = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bff89ac-0525-4d7a-9efe-d3e276bc002e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33912, 200), (33912, 9), (8597, 200), (8597, 9))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = create_sequences(train_set, valid_periods_train, window, telescope, stride)\n",
    "X_val, y_val = create_sequences(val_set, valid_periods_val, window, telescope, stride)\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0524d67-b39b-48fc-8a7b-3eb95ddcd344",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (window, 1)\n",
    "category_shape = (6, )\n",
    "output_shape = (telescope, 1)\n",
    "batch_size = 64\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22e5b403-7518-4921-aefd-57a78845711e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 21:18:52.595041: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-15 21:18:52.644586: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-15 21:18:53.403485: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from Transformer import Transformer\n",
    "model = Transformer(window, telescope, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c1603-0ba3-402d-8bc0-1e8fb55b941b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 21:18:54.251512: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-15 21:18:54.291321: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-15 21:18:54.293454: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-15 21:18:54.296056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-15 21:18:54.298063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-15 21:18:54.300047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-15 21:18:55.058882: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-15 21:18:55.059889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-15 21:18:55.060663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-15 21:18:55.061383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13635 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 200, 1)]             0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 200, 1)               2         ['input_1[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 200, 1)               7169      ['layer_normalization[0][0]', \n",
      " iHeadAttention)                                                     'layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 200, 1)               0         ['multi_head_attention[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 200, 1)               0         ['dropout[0][0]',             \n",
      " Lambda)                                                             'input_1[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 200, 1)               2         ['tf.__operators__.add[0][0]']\n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 200, 4)               8         ['layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 200, 4)               0         ['conv1d[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 200, 1)               5         ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 200, 1)               0         ['conv1d_1[0][0]',            \n",
      " OpLambda)                                                           'tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 200, 1)               2         ['tf.__operators__.add_1[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 200, 1)               7169      ['layer_normalization_2[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 200, 1)               0         ['multi_head_attention_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 200, 1)               0         ['dropout_2[0][0]',           \n",
      " OpLambda)                                                           'tf.__operators__.add_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 200, 1)               2         ['tf.__operators__.add_2[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 200, 4)               8         ['layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 200, 4)               0         ['conv1d_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 200, 1)               5         ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TF  (None, 200, 1)               0         ['conv1d_3[0][0]',            \n",
      " OpLambda)                                                           'tf.__operators__.add_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, 200, 1)               2         ['tf.__operators__.add_3[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (None, 200, 1)               7169      ['layer_normalization_4[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 200, 1)               0         ['multi_head_attention_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TF  (None, 200, 1)               0         ['dropout_4[0][0]',           \n",
      " OpLambda)                                                           'tf.__operators__.add_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, 200, 1)               2         ['tf.__operators__.add_4[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 200, 4)               8         ['layer_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 200, 4)               0         ['conv1d_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 200, 1)               5         ['dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TF  (None, 200, 1)               0         ['conv1d_5[0][0]',            \n",
      " OpLambda)                                                           'tf.__operators__.add_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 200, 1)               2         ['tf.__operators__.add_5[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (Mu  (None, 200, 1)               7169      ['layer_normalization_6[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 200, 1)               0         ['multi_head_attention_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TF  (None, 200, 1)               0         ['dropout_6[0][0]',           \n",
      " OpLambda)                                                           'tf.__operators__.add_5[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 200, 1)               2         ['tf.__operators__.add_6[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 200, 4)               8         ['layer_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 200, 4)               0         ['conv1d_6[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 200, 1)               5         ['dropout_7[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TF  (None, 200, 1)               0         ['conv1d_7[0][0]',            \n",
      " OpLambda)                                                           'tf.__operators__.add_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_8 (Lay  (None, 200, 1)               2         ['tf.__operators__.add_7[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (Mu  (None, 200, 1)               7169      ['layer_normalization_8[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_8[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 200, 1)               0         ['multi_head_attention_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TF  (None, 200, 1)               0         ['dropout_8[0][0]',           \n",
      " OpLambda)                                                           'tf.__operators__.add_7[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_9 (Lay  (None, 200, 1)               2         ['tf.__operators__.add_8[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, 200, 4)               8         ['layer_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)         (None, 200, 4)               0         ['conv1d_8[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)           (None, 200, 1)               5         ['dropout_9[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TF  (None, 200, 1)               0         ['conv1d_9[0][0]',            \n",
      " OpLambda)                                                           'tf.__operators__.add_8[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_10 (La  (None, 200, 1)               2         ['tf.__operators__.add_9[0][0]\n",
      " yerNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (Mu  (None, 200, 1)               7169      ['layer_normalization_10[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)        (None, 200, 1)               0         ['multi_head_attention_5[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (T  (None, 200, 1)               0         ['dropout_10[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_9[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_11 (La  (None, 200, 1)               2         ['tf.__operators__.add_10[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, 200, 4)               8         ['layer_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)        (None, 200, 4)               0         ['conv1d_10[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)          (None, 200, 1)               5         ['dropout_11[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (T  (None, 200, 1)               0         ['conv1d_11[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_10[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_12 (La  (None, 200, 1)               2         ['tf.__operators__.add_11[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (Mu  (None, 200, 1)               7169      ['layer_normalization_12[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)        (None, 200, 1)               0         ['multi_head_attention_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_12 (T  (None, 200, 1)               0         ['dropout_12[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_11[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_13 (La  (None, 200, 1)               2         ['tf.__operators__.add_12[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)          (None, 200, 4)               8         ['layer_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)        (None, 200, 4)               0         ['conv1d_12[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)          (None, 200, 1)               5         ['dropout_13[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_13 (T  (None, 200, 1)               0         ['conv1d_13[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_12[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_14 (La  (None, 200, 1)               2         ['tf.__operators__.add_13[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (Mu  (None, 200, 1)               7169      ['layer_normalization_14[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)        (None, 200, 1)               0         ['multi_head_attention_7[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_14 (T  (None, 200, 1)               0         ['dropout_14[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_13[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_15 (La  (None, 200, 1)               2         ['tf.__operators__.add_14[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)          (None, 200, 4)               8         ['layer_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)        (None, 200, 4)               0         ['conv1d_14[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)          (None, 200, 1)               5         ['dropout_15[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_15 (T  (None, 200, 1)               0         ['conv1d_15[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_14[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 200)                  0         ['tf.__operators__.add_15[0][0\n",
      " GlobalAveragePooling1D)                                            ]']                           \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 128)                  25728     ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)        (None, 128)                  0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 9)                    1161      ['dropout_16[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 84377 (329.60 KB)\n",
      "Trainable params: 84377 (329.60 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 21:19:07.616429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-12-15 21:19:08.425044: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562c5dcd2bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-15 21:19:08.425075: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2023-12-15 21:19:08.430580: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-12-15 21:19:08.556185: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "530/530 [==============================] - ETA: 0s - loss: 0.0323 - rmse: 0.1667 - mae: 0.1245 - smape: 16.4589 - coeff_determination: 0.5552\n",
      "Epoch 1: val_loss improved from inf to 0.01247, saving model to parameters/Transformer.best15122023_21:18:56.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/hive_retrieval_engine/test_venv/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "530/530 [==============================] - 146s 231ms/step - loss: 0.0323 - rmse: 0.1667 - mae: 0.1245 - smape: 16.4589 - coeff_determination: 0.5552 - val_loss: 0.0125 - val_rmse: 0.1091 - val_mae: 0.0725 - val_smape: 10.9043 - val_coeff_determination: 0.7962 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0158 - rmse: 0.1248 - mae: 0.0894 - smape: 12.2076 - coeff_determination: 0.7811\n",
      "Epoch 2: val_loss improved from 0.01247 to 0.01136, saving model to parameters/Transformer.best15122023_21:18:56.hdf5\n",
      "530/530 [==============================] - 122s 231ms/step - loss: 0.0158 - rmse: 0.1248 - mae: 0.0894 - smape: 12.2076 - coeff_determination: 0.7811 - val_loss: 0.0114 - val_rmse: 0.1039 - val_mae: 0.0693 - val_smape: 10.3557 - val_coeff_determination: 0.8141 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0138 - rmse: 0.1166 - mae: 0.0818 - smape: 11.4459 - coeff_determination: 0.8087\n",
      "Epoch 3: val_loss improved from 0.01136 to 0.01045, saving model to parameters/Transformer.best15122023_21:18:56.hdf5\n",
      "530/530 [==============================] - 122s 231ms/step - loss: 0.0138 - rmse: 0.1166 - mae: 0.0818 - smape: 11.4459 - coeff_determination: 0.8087 - val_loss: 0.0104 - val_rmse: 0.0994 - val_mae: 0.0646 - val_smape: 9.7575 - val_coeff_determination: 0.8295 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0129 - rmse: 0.1126 - mae: 0.0785 - smape: 11.2148 - coeff_determination: 0.8219\n",
      "Epoch 4: val_loss did not improve from 0.01045\n",
      "530/530 [==============================] - 122s 231ms/step - loss: 0.0129 - rmse: 0.1126 - mae: 0.0785 - smape: 11.2148 - coeff_determination: 0.8219 - val_loss: 0.0105 - val_rmse: 0.0998 - val_mae: 0.0669 - val_smape: 9.8516 - val_coeff_determination: 0.8290 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0123 - rmse: 0.1098 - mae: 0.0758 - smape: 10.9372 - coeff_determination: 0.8300\n",
      "Epoch 5: val_loss did not improve from 0.01045\n",
      "530/530 [==============================] - 122s 230ms/step - loss: 0.0123 - rmse: 0.1098 - mae: 0.0758 - smape: 10.9372 - coeff_determination: 0.8300 - val_loss: 0.0115 - val_rmse: 0.1053 - val_mae: 0.0749 - val_smape: 10.4120 - val_coeff_determination: 0.8112 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0120 - rmse: 0.1086 - mae: 0.0748 - smape: 10.9094 - coeff_determination: 0.8341\n",
      "Epoch 6: val_loss improved from 0.01045 to 0.00998, saving model to parameters/Transformer.best15122023_21:18:56.hdf5\n",
      "530/530 [==============================] - 122s 231ms/step - loss: 0.0120 - rmse: 0.1086 - mae: 0.0748 - smape: 10.9094 - coeff_determination: 0.8341 - val_loss: 0.0100 - val_rmse: 0.0970 - val_mae: 0.0617 - val_smape: 9.3744 - val_coeff_determination: 0.8371 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0116 - rmse: 0.1068 - mae: 0.0732 - smape: 10.7646 - coeff_determination: 0.8394\n",
      "Epoch 7: val_loss improved from 0.00998 to 0.00978, saving model to parameters/Transformer.best15122023_21:18:56.hdf5\n",
      "530/530 [==============================] - 122s 231ms/step - loss: 0.0116 - rmse: 0.1068 - mae: 0.0732 - smape: 10.7646 - coeff_determination: 0.8394 - val_loss: 0.0098 - val_rmse: 0.0960 - val_mae: 0.0616 - val_smape: 9.4074 - val_coeff_determination: 0.8407 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0114 - rmse: 0.1061 - mae: 0.0725 - smape: 10.7654 - coeff_determination: 0.8418\n",
      "Epoch 8: val_loss did not improve from 0.00978\n",
      "530/530 [==============================] - 122s 231ms/step - loss: 0.0114 - rmse: 0.1061 - mae: 0.0725 - smape: 10.7654 - coeff_determination: 0.8418 - val_loss: 0.0099 - val_rmse: 0.0967 - val_mae: 0.0618 - val_smape: 9.4917 - val_coeff_determination: 0.8381 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0112 - rmse: 0.1046 - mae: 0.0715 - smape: 10.6789 - coeff_determination: 0.8456\n",
      "Epoch 9: val_loss did not improve from 0.00978\n",
      "530/530 [==============================] - 122s 230ms/step - loss: 0.0112 - rmse: 0.1046 - mae: 0.0715 - smape: 10.6789 - coeff_determination: 0.8456 - val_loss: 0.0100 - val_rmse: 0.0973 - val_mae: 0.0650 - val_smape: 9.9120 - val_coeff_determination: 0.8377 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0109 - rmse: 0.1035 - mae: 0.0704 - smape: 10.5983 - coeff_determination: 0.8493\n",
      "Epoch 10: val_loss improved from 0.00978 to 0.00949, saving model to parameters/Transformer.best15122023_21:18:56.hdf5\n",
      "530/530 [==============================] - 122s 231ms/step - loss: 0.0109 - rmse: 0.1035 - mae: 0.0704 - smape: 10.5983 - coeff_determination: 0.8493 - val_loss: 0.0095 - val_rmse: 0.0947 - val_mae: 0.0611 - val_smape: 9.4848 - val_coeff_determination: 0.8450 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0109 - rmse: 0.1032 - mae: 0.0703 - smape: 10.6142 - coeff_determination: 0.8502\n",
      "Epoch 11: val_loss did not improve from 0.00949\n",
      "530/530 [==============================] - 122s 230ms/step - loss: 0.0109 - rmse: 0.1032 - mae: 0.0703 - smape: 10.6142 - coeff_determination: 0.8502 - val_loss: 0.0095 - val_rmse: 0.0947 - val_mae: 0.0603 - val_smape: 9.2845 - val_coeff_determination: 0.8451 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0106 - rmse: 0.1020 - mae: 0.0690 - smape: 10.4908 - coeff_determination: 0.8530\n",
      "Epoch 12: val_loss did not improve from 0.00949\n",
      "530/530 [==============================] - 122s 231ms/step - loss: 0.0106 - rmse: 0.1020 - mae: 0.0690 - smape: 10.4908 - coeff_determination: 0.8530 - val_loss: 0.0098 - val_rmse: 0.0965 - val_mae: 0.0635 - val_smape: 9.3133 - val_coeff_determination: 0.8398 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0105 - rmse: 0.1016 - mae: 0.0689 - smape: 10.5066 - coeff_determination: 0.8540\n",
      "Epoch 13: val_loss improved from 0.00949 to 0.00941, saving model to parameters/Transformer.best15122023_21:18:56.hdf5\n",
      "530/530 [==============================] - 122s 231ms/step - loss: 0.0105 - rmse: 0.1016 - mae: 0.0689 - smape: 10.5066 - coeff_determination: 0.8540 - val_loss: 0.0094 - val_rmse: 0.0943 - val_mae: 0.0611 - val_smape: 9.5330 - val_coeff_determination: 0.8467 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0104 - rmse: 0.1008 - mae: 0.0682 - smape: 10.4585 - coeff_determination: 0.8569\n",
      "Epoch 14: val_loss improved from 0.00941 to 0.00932, saving model to parameters/Transformer.best15122023_21:18:56.hdf5\n",
      "530/530 [==============================] - 122s 231ms/step - loss: 0.0104 - rmse: 0.1008 - mae: 0.0682 - smape: 10.4585 - coeff_determination: 0.8569 - val_loss: 0.0093 - val_rmse: 0.0940 - val_mae: 0.0599 - val_smape: 9.4103 - val_coeff_determination: 0.8479 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0103 - rmse: 0.1007 - mae: 0.0682 - smape: 10.4949 - coeff_determination: 0.8574\n",
      "Epoch 15: val_loss did not improve from 0.00932\n",
      "530/530 [==============================] - 122s 231ms/step - loss: 0.0103 - rmse: 0.1007 - mae: 0.0682 - smape: 10.4949 - coeff_determination: 0.8574 - val_loss: 0.0098 - val_rmse: 0.0963 - val_mae: 0.0636 - val_smape: 9.5531 - val_coeff_determination: 0.8406 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0103 - rmse: 0.1005 - mae: 0.0680 - smape: 10.4796 - coeff_determination: 0.8572\n",
      "Epoch 16: val_loss did not improve from 0.00932\n",
      "530/530 [==============================] - 122s 230ms/step - loss: 0.0103 - rmse: 0.1005 - mae: 0.0680 - smape: 10.4796 - coeff_determination: 0.8572 - val_loss: 0.0094 - val_rmse: 0.0944 - val_mae: 0.0606 - val_smape: 9.5572 - val_coeff_determination: 0.8463 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0102 - rmse: 0.0999 - mae: 0.0675 - smape: 10.4192 - coeff_determination: 0.8593\n",
      "Epoch 17: val_loss improved from 0.00932 to 0.00921, saving model to parameters/Transformer.best15122023_21:18:56.hdf5\n",
      "530/530 [==============================] - 122s 231ms/step - loss: 0.0102 - rmse: 0.0999 - mae: 0.0675 - smape: 10.4192 - coeff_determination: 0.8593 - val_loss: 0.0092 - val_rmse: 0.0931 - val_mae: 0.0596 - val_smape: 9.3753 - val_coeff_determination: 0.8500 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0101 - rmse: 0.0994 - mae: 0.0671 - smape: 10.4278 - coeff_determination: 0.8599\n",
      "Epoch 18: val_loss did not improve from 0.00921\n",
      "530/530 [==============================] - 122s 231ms/step - loss: 0.0101 - rmse: 0.0994 - mae: 0.0671 - smape: 10.4278 - coeff_determination: 0.8599 - val_loss: 0.0093 - val_rmse: 0.0937 - val_mae: 0.0609 - val_smape: 9.4153 - val_coeff_determination: 0.8484 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0100 - rmse: 0.0991 - mae: 0.0668 - smape: 10.4154 - coeff_determination: 0.8615\n",
      "Epoch 19: val_loss did not improve from 0.00921\n",
      "530/530 [==============================] - 122s 230ms/step - loss: 0.0100 - rmse: 0.0991 - mae: 0.0668 - smape: 10.4154 - coeff_determination: 0.8615 - val_loss: 0.0092 - val_rmse: 0.0934 - val_mae: 0.0600 - val_smape: 9.0324 - val_coeff_determination: 0.8497 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0099 - rmse: 0.0985 - mae: 0.0666 - smape: 10.3820 - coeff_determination: 0.8632\n",
      "Epoch 20: val_loss improved from 0.00921 to 0.00916, saving model to parameters/Transformer.best15122023_21:18:56.hdf5\n",
      "530/530 [==============================] - 122s 231ms/step - loss: 0.0099 - rmse: 0.0985 - mae: 0.0666 - smape: 10.3820 - coeff_determination: 0.8632 - val_loss: 0.0092 - val_rmse: 0.0929 - val_mae: 0.0586 - val_smape: 9.1209 - val_coeff_determination: 0.8504 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0098 - rmse: 0.0982 - mae: 0.0663 - smape: 10.3432 - coeff_determination: 0.8638\n",
      "Epoch 21: val_loss improved from 0.00916 to 0.00902, saving model to parameters/Transformer.best15122023_21:18:56.hdf5\n",
      "530/530 [==============================] - 122s 231ms/step - loss: 0.0098 - rmse: 0.0982 - mae: 0.0663 - smape: 10.3432 - coeff_determination: 0.8638 - val_loss: 0.0090 - val_rmse: 0.0924 - val_mae: 0.0595 - val_smape: 9.1593 - val_coeff_determination: 0.8529 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0098 - rmse: 0.0979 - mae: 0.0660 - smape: 10.3545 - coeff_determination: 0.8644\n",
      "Epoch 22: val_loss did not improve from 0.00902\n",
      "530/530 [==============================] - 122s 231ms/step - loss: 0.0098 - rmse: 0.0979 - mae: 0.0660 - smape: 10.3545 - coeff_determination: 0.8644 - val_loss: 0.0092 - val_rmse: 0.0934 - val_mae: 0.0611 - val_smape: 9.7208 - val_coeff_determination: 0.8499 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0097 - rmse: 0.0978 - mae: 0.0660 - smape: 10.3183 - coeff_determination: 0.8650\n",
      "Epoch 23: val_loss did not improve from 0.00902\n",
      "530/530 [==============================] - 122s 231ms/step - loss: 0.0097 - rmse: 0.0978 - mae: 0.0660 - smape: 10.3183 - coeff_determination: 0.8650 - val_loss: 0.0091 - val_rmse: 0.0929 - val_mae: 0.0605 - val_smape: 9.5173 - val_coeff_determination: 0.8514 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0097 - rmse: 0.0978 - mae: 0.0662 - smape: 10.3811 - coeff_determination: 0.8648\n",
      "Epoch 24: val_loss did not improve from 0.00902\n",
      "530/530 [==============================] - 122s 230ms/step - loss: 0.0097 - rmse: 0.0978 - mae: 0.0662 - smape: 10.3811 - coeff_determination: 0.8648 - val_loss: 0.0090 - val_rmse: 0.0924 - val_mae: 0.0585 - val_smape: 9.1370 - val_coeff_determination: 0.8527 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0096 - rmse: 0.0970 - mae: 0.0654 - smape: 10.2706 - coeff_determination: 0.8675\n",
      "Epoch 25: val_loss did not improve from 0.00902\n",
      "530/530 [==============================] - 122s 230ms/step - loss: 0.0096 - rmse: 0.0970 - mae: 0.0654 - smape: 10.2706 - coeff_determination: 0.8675 - val_loss: 0.0091 - val_rmse: 0.0923 - val_mae: 0.0580 - val_smape: 8.9397 - val_coeff_determination: 0.8521 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0096 - rmse: 0.0969 - mae: 0.0656 - smape: 10.3168 - coeff_determination: 0.8673\n",
      "Epoch 26: val_loss did not improve from 0.00902\n",
      "530/530 [==============================] - 122s 230ms/step - loss: 0.0096 - rmse: 0.0969 - mae: 0.0656 - smape: 10.3168 - coeff_determination: 0.8673 - val_loss: 0.0096 - val_rmse: 0.0955 - val_mae: 0.0631 - val_smape: 9.1619 - val_coeff_determination: 0.8425 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0096 - rmse: 0.0968 - mae: 0.0653 - smape: 10.3005 - coeff_determination: 0.8679\n",
      "Epoch 27: val_loss did not improve from 0.00902\n",
      "530/530 [==============================] - 122s 231ms/step - loss: 0.0096 - rmse: 0.0968 - mae: 0.0653 - smape: 10.3005 - coeff_determination: 0.8679 - val_loss: 0.0090 - val_rmse: 0.0925 - val_mae: 0.0601 - val_smape: 9.5519 - val_coeff_determination: 0.8524 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0095 - rmse: 0.0967 - mae: 0.0655 - smape: 10.2925 - coeff_determination: 0.8679\n",
      "Epoch 28: val_loss did not improve from 0.00902\n",
      "530/530 [==============================] - 122s 231ms/step - loss: 0.0095 - rmse: 0.0967 - mae: 0.0655 - smape: 10.2925 - coeff_determination: 0.8679 - val_loss: 0.0090 - val_rmse: 0.0925 - val_mae: 0.0603 - val_smape: 9.6178 - val_coeff_determination: 0.8523 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0096 - rmse: 0.0969 - mae: 0.0655 - smape: 10.3527 - coeff_determination: 0.8681\n",
      "Epoch 29: val_loss improved from 0.00902 to 0.00882, saving model to parameters/Transformer.best15122023_21:18:56.hdf5\n",
      "530/530 [==============================] - 122s 231ms/step - loss: 0.0096 - rmse: 0.0969 - mae: 0.0655 - smape: 10.3527 - coeff_determination: 0.8681 - val_loss: 0.0088 - val_rmse: 0.0911 - val_mae: 0.0576 - val_smape: 9.1840 - val_coeff_determination: 0.8562 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0095 - rmse: 0.0964 - mae: 0.0652 - smape: 10.3104 - coeff_determination: 0.8690\n",
      "Epoch 30: val_loss did not improve from 0.00882\n",
      "530/530 [==============================] - 122s 230ms/step - loss: 0.0095 - rmse: 0.0964 - mae: 0.0652 - smape: 10.3104 - coeff_determination: 0.8690 - val_loss: 0.0089 - val_rmse: 0.0916 - val_mae: 0.0572 - val_smape: 8.7262 - val_coeff_determination: 0.8547 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "530/530 [==============================] - ETA: 0s - loss: 0.0093 - rmse: 0.0958 - mae: 0.0645 - smape: 10.2582 - coeff_determination: 0.8706\n",
      "Epoch 31: val_loss did not improve from 0.00882\n",
      "530/530 [==============================] - 122s 230ms/step - loss: 0.0093 - rmse: 0.0958 - mae: 0.0645 - smape: 10.2582 - coeff_determination: 0.8706 - val_loss: 0.0089 - val_rmse: 0.0919 - val_mae: 0.0596 - val_smape: 9.5311 - val_coeff_determination: 0.8546 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "296/530 [===============>..............] - ETA: 49s - loss: 0.0095 - rmse: 0.0962 - mae: 0.0650 - smape: 10.3203 - coeff_determination: 0.8687"
     ]
    }
   ],
   "source": [
    "model.train(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb37c1e-2cee-4897-8d46-d627627dd0b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
